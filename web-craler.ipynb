# -----------------------------------------------------------
# üìì CELL¬†1 ‚Äì¬†System setup: APT packages, Ollama, CUDA drivers
# -----------------------------------------------------------
!sudo apt-get update -y
!sudo apt-get install -y curl pciutils lsb-release
!curl -fsSL https://ollama.com/install.sh | sh

# Optional: check GPU
!nvidia-smi || true

# -----------------------------------------------------------
# üìì CELL¬†2 ‚Äì¬†Python dependencies
# -----------------------------------------------------------
%%bash
pip install -q \
  langchain-core \
  langchain-community \
  langchain-chroma \
  langchain-ollama \
  sentence-transformers \
  chromadb \
  gradio \
  beautifulsoup4 \
  tqdm

# -----------------------------------------------------------
# üìì CELL¬†3 ‚Äì¬†Pull the LLM & start Ollama server
# -----------------------------------------------------------
import subprocess, threading, requests, time, os

LLM_MODEL      = "mistral:7b"
EMBED_MODEL    = "nomic-embed-text"


def _serve():
    subprocess.Popen(["ollama", "serve"],
                     stdout=subprocess.DEVNULL,
                     stderr=subprocess.DEVNULL)


a = threading.Thread(target=_serve, daemon=True)
a.start()

# Wait until Ollama REST endpoint is up
for _ in range(20):
    try:
        if requests.get("http://localhost:11434").ok:
            break
    except:
        time.sleep(1)
else:
    raise RuntimeError("‚ùå Ollama failed to start.")

!ollama pull {LLM_MODEL}
!ollama pull {EMBED_MODEL}

print("‚úÖ Ollama ready with:", LLM_MODEL, "and", EMBED_MODEL)

# -----------------------------------------------------------
# üìì CELL 4 ‚Äì Imports & global objects
# -----------------------------------------------------------

import warnings, uuid, os, asyncio, concurrent.futures
from pathlib import Path
from typing import List
from bs4 import BeautifulSoup

# LangChain imports
from langchain_community.document_loaders import RecursiveUrlLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.llms import Ollama
from langchain_ollama.embeddings import OllamaEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Embedder & splitter
EMBEDDER = OllamaEmbeddings(model="nomic-embed-text")
SPLITTER = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)

# Safe prompt: model must stick to context
SAFE_PROMPT = PromptTemplate(
    input_variables=["context", "question"],
    template=(
        "Use ONLY the context below to answer the question. "
        "If the answer is not in the context, reply exactly: "
        "'I don't know based on the provided website content.'\n\n"
        "Context:\n{context}\n\n"
        "Question: {question}\nAnswer:"
    )
)

warnings.filterwarnings("ignore")

# -----------------------------------------------------------
# üìì CELL 5 ‚Äì Utility functions (crawl site, build index, etc.)
# -----------------------------------------------------------
def bs4_extract(html: str) -> str:
    """Convert raw HTML to plain text using BeautifulSoup."""
    soup = BeautifulSoup(html, "html.parser")
    return soup.get_text(separator="\n")


def crawl_website(root_url: str, depth: int = 2):
    """Fetch pages under root_url up to 'depth' levels and return LangChain Docs."""
    print(f"üåê Crawling: {root_url} (depth={depth})")
    try:
        loader = RecursiveUrlLoader(
            url=root_url,
            max_depth=depth,
            extractor=bs4_extract,  # pass callable, not string -> avoids 'str' is not callable
            timeout=10,
        )
        docs = loader.load()
        print(f"‚úÖ Found {len(docs)} document(s)")
    except Exception as e:
        raise RuntimeError(f"RecursiveUrlLoader failed: {e}")

    for d in docs:
        d.metadata["source"] = d.metadata.get("source", root_url)
    return docs


def build_vectorstore(docs, sid):
    chunks = SPLITTER.split_documents(docs)
    return Chroma.from_documents(
        chunks, embedding=EMBEDDER,
        collection_name=f"session-{sid}", persist_directory=None
    )


def make_qa_chain(vstore):
    retriever = vstore.as_retriever(search_type="mmr", search_kwargs={"k": 4})
    llm = Ollama(model=LLM_MODEL)
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        return_source_documents=True,
        chain_type_kwargs={"prompt": SAFE_PROMPT},
    )

# Thread pool for concurrent LLM calls
EXECUTOR = concurrent.futures.ThreadPoolExecutor(max_workers=4)

# -----------------------------------------------------------
# üìì CELL¬†6 ‚Äì¬†Gradio helpers
# -----------------------------------------------------------

import gradio as gr


def ingest_fn(state, url, depth):
    url = (url or "").strip()
    if not url:
        return state, "‚ö†Ô∏è Please enter a website URL."

    if not (url.startswith("http://") or url.startswith("https://")):
        url = "https://" + url

    sid = state.setdefault("id", str(uuid.uuid4()))

    try:
        docs = crawl_website(url, depth=int(depth))
    except Exception as e:
        return state, f"‚ö†Ô∏è Crawl failed: {type(e).__name__}: {e}"

    if not docs:
        return state, "‚ö†Ô∏è Could not retrieve any pages."

    vstore = build_vectorstore(docs, sid)
    state["qa_chain"] = make_qa_chain(vstore)
    state["history"] = []
    return state, f"‚úÖ Indexed {len(docs)} page(s) from {url}. Ask away!"


async def chat_fn(state, user_msg):
    user_msg = (user_msg or "").strip()
    hist = state.get("history", [])

    if not user_msg:
        yield state, hist
        return

    if "qa_chain" not in state:
        hist.append((user_msg, "‚ö†Ô∏è Crawl a website first."))
        yield state, hist
        return

    hist.append((user_msg, "‚è≥ ‚Ä¶thinking‚Ä¶"))
    yield state, hist

    loop = asyncio.get_event_loop()
    try:
        res = await loop.run_in_executor(
            EXECUTOR, state["qa_chain"].invoke, {"query": user_msg}
        )
        answer = res["result"]
    except Exception as e:
        answer = f"‚ö†Ô∏è {type(e).__name__}: {e}"

    hist[-1] = (user_msg, answer)
    yield state, hist


def clear_chat(state):
    state["history"] = []
    return state, []


# -----------------------------------------------------------
# üìì CELL¬†7 ‚Äì¬†Multi‚Äësession Website Content QA Gradio app
# -----------------------------------------------------------

with gr.Blocks(title="Website Content QA Bot (multi‚Äëuser)") as demo:
    gr.Markdown("## üåê Enter a website URL & ask questions ‚Äî isolated per session (parallel answers)")

    with gr.Row():
        url_box = gr.Textbox(label="Website URL", placeholder="https://example.com")
        depth_slider = gr.Slider(label="Crawl depth", value=2, minimum=1, maximum=3, step=1)
        idx_btn = gr.Button("Index website", variant="primary")
    status = gr.Markdown()

    chatbot   = gr.Chatbot(label="Chatbot", height=430)
    user_box  = gr.Textbox(label="Your question", placeholder="Type and hit Enter‚Ä¶")
    clear_btn = gr.Button("Clear chat")

    session_state = gr.State({})

    idx_btn.click(
        ingest_fn, inputs=[session_state, url_box, depth_slider], outputs=[session_state, status]
    )
    user_box.submit(
        chat_fn, inputs=[session_state, user_box], outputs=[session_state, chatbot]
    )
    clear_btn.click(
        clear_chat, inputs=session_state, outputs=[session_state, chatbot]
    )

# Run the app
if __name__ == "__main__":
    demo.queue(max_size=32).launch(share=True, debug=True)
